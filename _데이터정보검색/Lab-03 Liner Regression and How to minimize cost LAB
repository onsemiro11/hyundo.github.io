이번에는 tensorflow로 비용 최소, cost 최소화 하는것을 살펴보겠습니다.
hypothesis를 단순화시켜 H(x) = Wx로 두고 실행해보겠습니다.

import tensorflow as tf
tf.enable_eager_execution()
import numpy as np      #이번에는 numpy모듈도 함께 불러와 실행하였다.
x = np.array([1,2,3])   #데이터는 x,y로 1,2,3 / 1,2,3 입력하였습니다.
y = np.array([1,2,3])
      
def cost_func(w,x,y):   #cost_func함수를 만듭니다.
    hypothesis = x*y    #단순화시킨 hypotesis의 공식인 x * y를 정의하고
    return tf.reduce_mean(tf.square(hypothesis - y))    #hypothesis에서 y를 뺀 값을 제곱한 후 모두 더해 평균을 정하는 함수가 cost_funcdlqslek.
w_values = np.linspace(-3,5,num=15)         #numpy의 linspace를 활용하여 -3부터 5를 15개로 나누의 w_values에 리스트값으로 넣습니다. 
cost_values=[]
for feed_w in w_values:                 #w_values 리스트에 있는 값을 for문의 범위에 두어 하나하나 요소를 뽑아내어 feed_w값으로 넣습니다.
    curr_cost = cost_func(feed_w,x,y)   #그 feed_w값을 cost_func함수에 넣어 실행 시켜 나온 값을 curr)cost로 지정한다.
    cost_values.append(curr_cost)       #그 후 curr_cost 값을 cost_values 리스트에 추가시킨다.
    print("{:6.3f} | {:10.5f}".format(feed_w , curr_cost))    #값들을 출력해봅니다.
    
<결과값> -  w에 따라 cost값이 2차함수 모양의 그래프를 그립니다..
#feed_w     curr_cost
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
 Gradient descent 기울기 하강을 유도한 cost함수의 값과 w의 값을 확인해봅시다.
 
 tf.set_random_seed(0)        #랜덤시드를 초기화 시키는 함수이다. 다시 수행했을때도 실행 가능하게 초기화 시킵니다.
 x_data = [1.,2.,3.,4.]       #x_data 와 y_data를 지정한다.
 y_data = [1.,3.,5.,7.]
 w=tf.Variable(tf.random_normal([1],-100.,100.))    #정규분포에 따르는 랜덤 넘버를 1개로 변수를 만들어서 w에 지정합니다.
 for step in range(300):      #아래 코드들을 300번 진행을 합니다..
    hypothesis = w* x         #hypothesis를 정의하고
    cost = tf.reduce_mean(tf.square(hypothesis - y))    #cost함수를 공식에 맞게 정의합니다..
    
    alpha = 0.01
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(w,x)-y,x)) #gradient, 기울기를 만드는 공식으로 cost함수를 미분시켜 만듭니다.
    descent=w - tf.multiply(alpha,gradient)       #cost함수를 미분시킽 값어 알파 값을 곱한후, w값에서 빼줍니다.
                                                  #그렇게 만들어진 값이 descent이고 이제 부터 w의 값은 descent로 지정됩니다. 
    w.assign(descent)           #descent를 w로 할당시킵니다.
    if step % 10 == 0:
        print('{:5}|{:10.4f}|{:10.6f}'.format(step,cost.numpy(),w.numpy()[0]))
                        #결과값을 10번에 한번씩 확인할 수 있도록 위와같은 형태로 출력하게 합니다.
                        
    <결과값> - cost값이 0으로 수렴해가는 것을 알 수 있습니다. w의 값또한 어떤 값(랜덤 값)이었다가 특정한 값으로 수렴해가는 것을 볼 수 있습니다.
#step     cost        w  
    0|11716.3086| 48.767971
   10| 4504.9126| 30.619968
   20| 1732.1364| 19.366755
   30|  666.0052| 12.388859
   40|  256.0785|  8.062004
   50|   98.4620|  5.379007
   60|   37.8586|  3.715335
   70|   14.5566|  2.683725
   80|    5.5970|  2.044044
   90|    2.1520|  1.647391
  100|    0.8275|  1.401434
  110|    0.3182|  1.248922
  120|    0.1223|  1.154351
  130|    0.0470|  1.095710
  140|    0.0181|  1.059348
  150|    0.0070|  1.036801
  160|    0.0027|  1.022819
  170|    0.0010|  1.014150
  180|    0.0004|  1.008774
  190|    0.0002|  1.005441
  200|    0.0001|  1.003374
  210|    0.0000|  1.002092
  220|    0.0000|  1.001297
  230|    0.0000|  1.000804
  240|    0.0000|  1.000499
  250|    0.0000|  1.000309
  260|    0.0000|  1.000192
  270|    0.0000|  1.000119
  280|    0.0000|  1.000074
  290|    0.0000|  1.000046
  
  #w값에 특정한 값을 주어도 특정한 한 값으로 수렴해감을 알 수 있을 겁니다. 
    한번, w값을 특정값으로 입력하고 변형되는 결과를 확인해보십시오.
